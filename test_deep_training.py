#!/usr/bin/env python3
"""
Test script for Deep MWNN Training
Tests the training pipeline with optimized batch sizes
"""

import torch
import torch.nn as nn
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import sys
import time

sys.path.append('.')

from src.models.continuous_integration.model import ContinuousIntegrationModel
from setup_colab import get_gpu_info, get_optimal_settings
from train_deep_colab import detect_optimal_batch_size, create_deep_model


def test_training_pipeline():
    """Test the complete training pipeline with proper batch sizes"""
    
    print("üß™ Testing Deep MWNN Training Pipeline")
    
    # Setup device (force CPU for local testing)
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"üñ•Ô∏è  Device: {device}")
    
    # Get optimal settings
    if device.type == 'cuda':
        gpu_info = get_gpu_info()
        optimal_settings = get_optimal_settings(gpu_info)
    else:
        optimal_settings = {
            'batch_size': 16,
            'num_workers': 2,
            'mixed_precision': False
        }
    
    # Create model (ImageNet-scale with 1000 classes)
    model = create_deep_model(num_classes=1000, complexity='medium')
    model.device = device  # Override auto-detection
    model = model.to(device)
    
    # Count parameters
    total_params = sum(p.numel() for p in model.parameters())
    print(f"üìä Model Parameters: {total_params:,}")
    
    # Test batch size optimization (CPU will be limited)
    if device.type == 'cuda':
        optimal_batch = detect_optimal_batch_size(model, device)
    else:
        optimal_batch = 8  # Conservative for CPU
    
    print(f"üì¶ Using batch size: {optimal_batch}")
    
    # Create sample dataset (CIFAR-100 scaled to ImageNet size)
    transform = transforms.Compose([
        transforms.Resize(224),  # ImageNet size
        transforms.RandomHorizontalFlip(0.5),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                           std=[0.229, 0.224, 0.225])
    ])
    
    print("üìÅ Loading CIFAR-100 dataset (as ImageNet proxy)...")
    trainset = torchvision.datasets.CIFAR100(root='./data', train=True,
                                           download=True, transform=transform)
    
    # Use smaller subset for testing
    subset_size = min(1000, len(trainset))
    trainset = torch.utils.data.Subset(trainset, range(subset_size))
    
    trainloader = DataLoader(trainset, batch_size=optimal_batch, 
                           shuffle=True, num_workers=2)
    
    print(f"‚úÖ Dataset loaded: {len(trainset)} samples")
    
    # Setup training
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001, weight_decay=0.01)
    
    # Test training loop
    print("\nüöÄ Testing Training Loop (3 batches)...")
    
    model.train()
    total_time = 0
    
    for batch_idx, (inputs, labels) in enumerate(trainloader):
        if batch_idx >= 3:  # Only test 3 batches
            break
            
        start_time = time.time()
        
        inputs, labels = inputs.to(device), labels.to(device)
        
        # Extract RGB and brightness for MWNN
        rgb_inputs = inputs
        brightness_inputs = 0.299 * inputs[:, 0:1] + 0.587 * inputs[:, 1:2] + 0.114 * inputs[:, 2:3]
        
        optimizer.zero_grad()
        
        # Forward pass
        outputs = model(rgb_inputs, brightness_inputs)
        loss = criterion(outputs, labels)
        
        # Backward pass
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        
        batch_time = time.time() - start_time
        total_time += batch_time
        
        # Calculate accuracy
        _, predicted = torch.max(outputs.data, 1)
        accuracy = 100 * (predicted == labels).sum().item() / labels.size(0)
        
        print(f"   Batch {batch_idx+1}: Loss = {loss.item():.4f}, "
              f"Acc = {accuracy:.2f}%, Time = {batch_time:.2f}s")
    
    avg_time = total_time / 3
    throughput = optimal_batch / avg_time
    
    print(f"\nüìà Performance Summary:")
    print(f"   Average batch time: {avg_time:.2f}s")
    print(f"   Throughput: {throughput:.1f} samples/sec")
    print(f"   Memory efficient: ‚úÖ")
    print(f"   Gradient flow: ‚úÖ")
    
    # Test model save/load
    print("\nüíæ Testing model checkpointing...")
    
    checkpoint = {
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'batch_size': optimal_batch,
        'total_params': total_params
    }
    
    checkpoint_path = 'checkpoints/test_deep_model.pth'
    torch.save(checkpoint, checkpoint_path)
    print(f"‚úÖ Checkpoint saved: {checkpoint_path}")
    
    # Verify loading
    loaded_checkpoint = torch.load(checkpoint_path, map_location=device)
    model.load_state_dict(loaded_checkpoint['model_state_dict'])
    print("‚úÖ Checkpoint loaded successfully")
    
    return {
        'optimal_batch_size': optimal_batch,
        'throughput': throughput,
        'total_params': total_params,
        'device': str(device),
        'test_passed': True
    }


def test_batch_size_scaling():
    """Test how batch size affects performance"""
    
    print("\nüî¨ Testing Batch Size Scaling...")
    
    device = torch.device('cpu')  # Force CPU for consistent testing
    model = create_deep_model(num_classes=100, complexity='shallow')  # Smaller for testing
    model.device = device
    model = model.to(device)
    
    batch_sizes = [4, 8, 16, 32]
    results = []
    
    for batch_size in batch_sizes:
        print(f"\n   Testing batch size: {batch_size}")
        
        try:
            # Create dummy data
            rgb_data = torch.randn(batch_size, 3, 224, 224, device=device)
            brightness_data = torch.randn(batch_size, 1, 224, 224, device=device)
            labels = torch.randint(0, 100, (batch_size,), device=device)
            
            # Time forward pass
            start_time = time.time()
            outputs = model(rgb_data, brightness_data)
            forward_time = time.time() - start_time
            
            # Time backward pass
            criterion = nn.CrossEntropyLoss()
            loss = criterion(outputs, labels)
            
            start_time = time.time()
            loss.backward()
            backward_time = time.time() - start_time
            
            total_time = forward_time + backward_time
            throughput = batch_size / total_time
            
            results.append({
                'batch_size': batch_size,
                'forward_time': forward_time,
                'backward_time': backward_time,
                'total_time': total_time,
                'throughput': throughput,
                'success': True
            })
            
            print(f"      ‚úÖ Forward: {forward_time:.3f}s, Backward: {backward_time:.3f}s")
            print(f"      üìà Throughput: {throughput:.1f} samples/sec")
            
            model.zero_grad()
            
        except Exception as e:
            print(f"      ‚ùå Failed: {e}")
            results.append({
                'batch_size': batch_size,
                'success': False,
                'error': str(e)
            })
    
    # Summary
    print(f"\nüìä Batch Size Scaling Summary:")
    successful_results = [r for r in results if r.get('success', False)]
    
    if successful_results:
        best_throughput = max(successful_results, key=lambda x: x['throughput'])
        print(f"   Best throughput: {best_throughput['throughput']:.1f} samples/sec "
              f"at batch size {best_throughput['batch_size']}")
        
        for result in successful_results:
            efficiency = result['throughput'] / best_throughput['throughput'] * 100
            print(f"   Batch {result['batch_size']:2d}: {efficiency:5.1f}% efficiency")
    
    return results


if __name__ == "__main__":
    print("üß™ Deep MWNN Training Tests")
    print("="*50)
    
    # Test 1: Full training pipeline
    try:
        pipeline_results = test_training_pipeline()
        print("\n‚úÖ Training pipeline test passed!")
    except Exception as e:
        print(f"\n‚ùå Training pipeline test failed: {e}")
        pipeline_results = None
    
    # Test 2: Batch size scaling
    try:
        scaling_results = test_batch_size_scaling()
        print("\n‚úÖ Batch size scaling test passed!")
    except Exception as e:
        print(f"\n‚ùå Batch size scaling test failed: {e}")
        scaling_results = None
    
    print(f"\n{'='*50}")
    print("üéØ Test Results Summary")
    print(f"{'='*50}")
    
    if pipeline_results:
        print(f"Pipeline Test: ‚úÖ")
        print(f"  Optimal Batch Size: {pipeline_results['optimal_batch_size']}")
        print(f"  Throughput: {pipeline_results['throughput']:.1f} samples/sec")
        print(f"  Device: {pipeline_results['device']}")
    else:
        print("Pipeline Test: ‚ùå")
    
    if scaling_results:
        successful_count = sum(1 for r in scaling_results if r.get('success', False))
        print(f"Scaling Test: ‚úÖ ({successful_count}/{len(scaling_results)} batch sizes)")
    else:
        print("Scaling Test: ‚ùå")
    
    print("\nüöÄ Ready for Colab deployment!")
