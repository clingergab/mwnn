# ImageNet MWNN Training - Colab Deployment Guide

## ğŸ¯ Overview

This guide provides everything needed to train Multi-Weight Neural Networks (MWNN) on ImageNet-1K using Google Colab with optimal batch sizes and GPU acceleration.

## ğŸ“¦ Package Contents

The `mwnn_colab_package.tar.gz` includes:

### Core Training Files
- `train_deep_colab.py` - Main ImageNet training script with auto batch size detection
- `setup_imagenet_colab.py` - Colab environment setup for ImageNet
- `optimize_batch_sizes.py` - GPU-specific batch size optimization
- `MWNN_Colab_Training.ipynb` - Complete training notebook

### MWNN Implementation
- `src/models/continuous_integration/` - Core MWNN architecture
- `src/preprocessing/imagenet_*` - ImageNet data loading and preprocessing
- `configs/` - Training configurations

### Supporting Scripts
- `setup_colab.py` - General Colab setup
- `test_*.py` - Validation and testing scripts
- Documentation and analysis files

## ğŸš€ Quick Start (5 Steps)

### 1. Upload to Colab
```bash
# In Colab, upload mwnn_colab_package.tar.gz
# Then extract:
!tar -xzf mwnn_colab_package.tar.gz
%cd multi-weight-neural-networks
```

### 2. Setup Environment
```python
!python setup_imagenet_colab.py
```

### 3. Download ImageNet
```python
# Method A: Kaggle API (recommended)
!pip install kaggle
# Upload kaggle.json, then:
!kaggle competitions download -c imagenet-object-localization-challenge

# Method B: Manual upload to /content/data/ImageNet-1K/
```

### 4. Verify Setup
```python
import torch
from train_deep_colab import run_deep_training

print(f"CUDA available: {torch.cuda.is_available()}")
print(f"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}")
```

### 5. Start Training
```python
# Quick start - automatically detects optimal batch size
results = run_deep_training(
    dataset_name='ImageNet',
    data_dir='/content/data/ImageNet-1K',
    complexity='deep',
    epochs=30,
    use_auto_batch_size=True
)
```

## âš™ï¸ Training Configuration

### Automatic Optimization
- **Batch Size**: Auto-detected based on GPU memory
- **Mixed Precision**: Enabled on compatible GPUs
- **Learning Rate**: Adaptive based on model complexity
- **Memory Management**: Gradient checkpointing for large models

### Model Complexities
- `shallow`: 288K parameters, batch size 128-256
- `medium`: 2.4M parameters, batch size 64-128  
- `deep`: 45M parameters, batch size 16-64

### Expected Performance
- **T4 GPU**: Batch size 16-32, ~2 hours/epoch
- **V100 GPU**: Batch size 64-128, ~1 hour/epoch
- **A100 GPU**: Batch size 128-256, ~30 min/epoch

## ğŸ“Š Monitoring Training

### Checkpoints
Training automatically saves:
- Best model: `/content/checkpoints/best_imagenet_model.pth`
- Regular checkpoints every 5 epochs
- Training history and metrics

### Logs
- TensorBoard logs: `/content/logs/`
- Training progress: Real-time in notebook
- Error logs: Captured and displayed

### Results
Final results saved to:
- `/content/results/imagenet_training_results.json`
- Includes accuracy, training time, and model info

## ğŸ¯ Expected Results

### MWNN on ImageNet-1K
- **Target Accuracy**: 75-80% Top-1 validation accuracy
- **Training Time**: 20-40 hours (30 epochs)
- **Memory Usage**: 8-16GB GPU memory
- **Convergence**: Typically by epoch 20-25

### Architecture Benefits
- **Dual-path processing**: RGB + Luminance channels
- **Efficient feature integration**: Multi-weight fusion
- **Robust gradients**: Continuous integration design
- **Memory efficient**: Gradient checkpointing

## ğŸ”§ Troubleshooting

### Common Issues

**GPU Out of Memory**
```python
# Reduce batch size
results = run_deep_training(
    complexity='medium',  # Smaller model
    use_auto_batch_size=False,
    batch_size=16  # Manual override
)
```

**Dataset Not Found**
```bash
# Verify ImageNet structure
!ls /content/data/ImageNet-1K/
# Should show: train/, val/, ILSVRC2013_devkit/
```

**Slow Training**
```python
# Use mixed precision
# Enable in GPU settings: Runtime > Change runtime type > Hardware accelerator > GPU
```

### Performance Tips

1. **Use Colab Pro**: Longer runtime, better GPUs
2. **Enable GPU**: Runtime > Change runtime type > GPU
3. **Monitor usage**: Check GPU memory with `!nvidia-smi`
4. **Save frequently**: Checkpoints saved automatically
5. **Use subsets**: For testing, use `load_subset=1000`

## ğŸ“ˆ Advanced Configuration

### Custom Training
```python
results = run_deep_training(
    dataset_name='ImageNet',
    data_dir='/content/data/ImageNet-1K',
    complexity='deep',
    epochs=50,
    learning_rate=0.0001,
    weight_decay=0.02,
    use_auto_batch_size=True,
    load_subset=None,  # Full dataset
    save_checkpoints=True
)
```

### Experiment Variations
```python
# Test different complexities
for complexity in ['shallow', 'medium', 'deep']:
    results = run_deep_training(
        complexity=complexity,
        epochs=10,
        load_subset=5000  # Quick test
    )
```

## ğŸ‰ Success Indicators

âœ… **Setup Complete**: Environment loads without errors  
âœ… **Dataset Ready**: ImageNet structure verified  
âœ… **GPU Active**: CUDA available and memory allocated  
âœ… **Training Started**: First epoch completes successfully  
âœ… **Batch Size Optimal**: No OOM errors, good throughput  
âœ… **Convergence**: Loss decreasing, accuracy improving  

## ğŸ“š Additional Resources

- **MWNN Paper**: [Link to research paper]
- **ImageNet Info**: https://image-net.org/
- **Colab Documentation**: https://colab.research.google.com/
- **PyTorch Docs**: https://pytorch.org/docs/

## ğŸ Final Notes

This setup provides a complete, optimized training pipeline for MWNN on ImageNet. The automatic batch size detection and GPU optimization ensure efficient training on any Colab GPU configuration.

**Estimated Total Time**: 1-2 days for full training  
**Expected Results**: State-of-the-art accuracy on ImageNet  
**GPU Requirements**: T4/V100/A100 (Colab Pro recommended)  

Happy training! ğŸš€
